# -*- coding: utf-8 -*-
"""K-Means clustering .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Da1xXFksH2P68FWFixuzMgRGj8Pzu5z_

**K-Means clustering algorithm to a dataset about Facebook live sellers in Thailand.**

DataSetLink: https://archive.ics.uci.edu/dataset/488/facebook+live+sellers+in+thailand

1. **Data Preprocessing**:
   - Imported necessary libraries like `numpy`, `pandas`, `matplotlib`, and `seaborn`.
   - Loaded the dataset, inspected the shape (7050 instances, 16 attributes).
   - Explore categorical variables (`status_id`, `status_published`, `status_type`) and dropped `status_id` and `status_published` because they are unique identifiers for each instance.
   - Convert the categorical `status_type` into integer values using `LabelEncoder`.

2. **Feature Scaling**:
   - Applied `MinMaxScaler` to scale the feature vector to a range between 0 and 1, ensuring that all features have the same scale for the clustering model.

3. **K-Means Clustering**:
   - Initially, you applied K-Means clustering with 2 clusters
   - You used the elbow method to determine the optimal number of clusters.
   - You then tested different numbers of clusters (3 and 4).
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("Live_20210128.csv")
print("Shape of the dataset:", data.shape)
print("Columns in the dataset:", data.columns)

data = data.dropna(axis=1, how='all')

data = data.drop(["status_id", "status_published"], axis=1)

label_encoder = LabelEncoder()
data['status_type'] = label_encoder.fit_transform(data['status_type'])

print("Missing values:\n", data.isnull().sum())

label_encoder = LabelEncoder()
data['status_type'] = label_encoder.fit_transform(data['status_type'])

print("Missing values:\n", data.isnull().sum())

# Step 8: Scale the Features
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

inertia = []
cluster_range = range(1, 11)  # Test 1 to 10 clusters
for k in cluster_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(cluster_range, inertia, marker='o', linestyle='--')
plt.title("Elbow Method for Optimal Clusters")
plt.xlabel("Number of Clusters")
plt.ylabel("Inertia")
plt.xticks(cluster_range)
plt.show()

optimal_clusters = 3
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
data['Cluster'] = kmeans.fit_predict(scaled_data)

plt.figure(figsize=(10, 6))
sns.scatterplot(
    x=data['num_reactions'],
    y=data['num_comments'],
    hue=data['Cluster'],
    palette="viridis",
    s=100
)

plt.title("Clusters based on 'num_reactions' and 'num_comments'")
plt.xlabel("Number of Reactions")
plt.ylabel("Number of Comments")
plt.legend(title="Cluster")
plt.show()

cluster_summary = data.groupby("Cluster").mean()
print("\nCluster Summary:\n", cluster_summary)

data.to_csv("Clustered_Live_20210128.csv", index=False)
print("Clustered data saved as 'Clustered_Live_20210128.csv'")